---
title: Machine Learning Lab
date: 2017-11-15 12:10:00 +0300
tags: [ML, Phython, Tensorflow]
---

### 게임 (문제, 환경)
    가. 프로즌 레이크
        (1) 디터미니스틱 : 액션에 따른 상태가 정해져 있다.
        (2) 논디터미니스틱 : 액션에 따른 상태가 일정하지 않다.
    나. 카트폴

### 상태, 액션, 보상 의 집합

### 보상의 정의
    가. Q Table : 16 * 4 행렬
    나. 뉴럴 넷 : 결과를 알 수 없는 박스

### ML 학습구조
    가. X, Y 가 있을 때, 박스 함수 F 를 알 수 없고,
    나. 여러 X, Y 데이터 값을 이용해 F를 찾는 것.
    다. 정답셋은 시뮬레이션을 통해 데이터를 축적한다. 즉, 시뮬레이션 -> 학습 의 Iteration

### Python, Tensorflow, Numpy
    가. Tensorflow : 네트워크를 설정하고, 변수 선언 후, 리소스를 활용해 네트워크를 실행하면서 학습한다.
    나. Numpy : Array
        (1) 벡터로 표현 후, 연산을 하는데.
        (2) 연산은 Array 와 매트릭스와 텐서를 활용한 연산을 도와주는 Library
        (3) 스칼라 값 생성을 도와줌.

### 카트풀 : 자동차 위에 막대기가 있어서, 쓰러지면 죽는 게임. 죽지 않게 움직인다.
    가. 상태 : 
        (1) 막대기와 카트 사이의 각도
        (2) 막대기가 움직일 때의 속도
        (3) 카트의 속도
        (4) 카트의 포지션

    나. 액션 : 앞, 뒤

    다. 리워드 : 쓰러짐, 안쓰러짐

### https://github.com/hunkim/ReinforcementZeroToAll 다운
    - 07_1_q_net_cartpole.py
    가. gym : 게임들의 환경
    나. learning_rate : 최적화 시키는 줄어드는 간격?
    다. input_size : state 갯수 4개
    라. output_size : action의 갯수 2개
    마. X : input 을 받아들이는 틀 => 학습데이터는 None이라고 정의 얼마나 될지 모르기 때문, 학습데이터와 input_size를 넣어줌.
        W : input 과 output의 정의에서 네트워크 연결 => 4,2의 매트릭스
        Qpred : 예측값, 함수 F의 출력값 => X 벡터와 W 매트릭스 간의 곱을 하겠다. (tf.matmul(X, W1))
        Y : 정답
    바. 전에는 테이블로 표현하던 정보들을 네트워크 노드와 엣지만으로 기억 할 수 있음. => 리소스가 훨씬 적게 듦.
    사. relu 는 벡터가 출력되기 전에 relu라는 함수를 거쳐서 나옴. => 0보다 작을 때는 0으로 0보다 클 때는 그대로 노출, 딥러닝에서 생기는 문제를 해결해주는 함수.
    아. 하이퍼파라미터 : 사람이 정의해주는 함수. relu가 그런거임. 써도 되고, 안써도 되고.
    자. loss : 테이블이 말하는 것처럼 네트워크가 같게 출력하면 좋다.
    차. 옵티마이저 : 최적화 값을 실행시켜주는 것으로 훈련한다. learning_rate 크기만큼.
    카. 에피소드 : 몇 번이나 게임을 해서 학습을 시킬지
    타. 디스카운트 : 액션의 시퀀스가 많아지면 안좋은거라고 판단하기 때문에 줄여주는 하이퍼파라미터
    파. 리소스 가져오는 걸 초기화 시키고, sess 라는 걸 통해서 벡터와 매트릭스 연산을 한다.
    하. e : 학습 진행 시, 익스플로네이션(랜덤으로 다 모으는 것), 익스플로잇(내가 학습시킨 애의 값을 사용하는 것)
        (1) 에피소드 값이 커지면 커질 수록 e 값이 작아짐. 작아지면 랜덤한 행동을 덜하게 됨. 학습시킨 네트워크 값을 믿자.